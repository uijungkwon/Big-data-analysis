/*네이버 뉴스에서 칼럼 가져오기*/
/*네이버 뉴스 > 오피니언 > 칼럼*/

from bs4 import BeautifulSoup
import requests
인터넷주소 = 'https://news.naver.com/opinion/column'
헤더 = {'User-Agent':"Mozilla/5.0(Windows NT 10.0; Win64; x64) AppleWebKit/537.3"}

오픈주소 = requests.get(인터넷주소,headers=헤더)
칼럼수프 = BeautifulSoup(오픈주소.text, 'html.parser')

/*기사 제목과 연결된 URL 가져오기*/
칼럼정보 = 칼럼수프.select("div.opinion_section li.opinion_column_item a")
print(len(칼럼정보))
for 칼럼 in 칼럼정보:
  print("사이트:", 칼럼['href'])

for 칼럼 in 칼럼수프.find_all('a',{'class': 'link'}):
  print("사이트:", 칼럼['href'])
  
for 사이트 in 칼럼수프.select("a"):
  print(사이트['href'])
  
칼럼정보 = 칼럼수프.select("div.opinion_section li.opinion_column_item a" )
print(len(칼럼정보))
print("1:", 칼럼정보[0])
print("2:", 칼럼정보[0].text)
print("3:", 칼럼정보[0].select_one("a")['href'])
print("4:", 칼럼정보[0].select_one("strong.title").string)

칼럼사전 = {}
for 칼럼 in 칼럼정보:
  print("제목:", 칼럼.select_one("strong.title").string)
  print("주소:", 칼럼.select_one("a").attrs['href'])
  칼럼사전[칼럼.select_one("a").attrs['href']] = 칼럼.select_one("strong.title")
칼럼사전 

통합칼럼 = ""
for 사이트, 제목 in 칼럼사전.items():
  통합칼럼 += "*"*10 + 제목 + "*"*10 + "\n"
  통합칼럼 += "URL: " + 사이트 + "\n"
  칼럼주소 = resquests.get(사이트,headers=헤더)
  칼럼수프 = BeautifulSoup(칼럼주소.text, 'html.parser')
  본문 = 칼럼수프.select('#dic_area')
  통합칼럼 += str(본문)+"\n"
print(통합칼럼)

통합칼럼 = 통합칼럼.replace("<br/>", "\n")
통합칼럼 = 통합칼럼.replace("</strong>", "\n")
통합칼럼 = 통합칼럼.replace("</span>", "\n")
통합칼럼 = 통합칼럼.replace("</em>", "\n")
통합칼럼 = 통합칼럼.replace("</table>", "\n")
통합칼럼 = 통합칼럼.replace("</b>", "\n")
통합칼럼 = 통합칼럼.replace("<function _flash_removeCallback() {}", "\n")

print(통합칼럼)

칼럼리스트 = 통합칼럼.split("\n")
칼럼리스트

제거문자 = ["[", "<", "/", "-", "©", "&", "▶", "◎", "<"]

칼럼묶음 = ""
for 문자열 in 칼럼리스트:
  문자열 = 문자열.lstrip()
  if 문자열 == "" : continue
  if 문자열[0] not in 제거문자: 칼럼묶음 += 문자열 + "\n"
print(칼럼묶음)




